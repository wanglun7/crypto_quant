"""
å†å²æ•°æ®è´¨é‡éªŒè¯æµ‹è¯•
éªŒè¯ä¸‹è½½çš„å†å²æ•°æ®æ˜¯å¦å®Œæ•´ã€å‡†ç¡®ï¼Œèƒ½å¦æ”¯æŒå›æµ‹
"""

import asyncio
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
import logging
import sys
import os
import zipfile
import io
from typing import Dict, Any

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from data_pipeline.historical_downloader import BinanceHistoricalDownloader, DataInterval

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class HistoricalDataValidator:
    """å†å²æ•°æ®éªŒè¯å™¨"""
    
    def __init__(self, data_path: str = "./data/test_basic"):
        self.data_path = Path(data_path)
        self.downloader = BinanceHistoricalDownloader(str(self.data_path))
        
    def parse_klines_file(self, file_path: Path) -> pd.DataFrame:
        """è§£æKçº¿æ•°æ®æ–‡ä»¶"""
        try:
            with zipfile.ZipFile(file_path, 'r') as zip_file:
                # è·å–CSVæ–‡ä»¶
                csv_files = [f for f in zip_file.namelist() if f.endswith('.csv')]
                if not csv_files:
                    logger.error(f"No CSV files found in {file_path}")
                    return pd.DataFrame()
                
                # è¯»å–CSVæ•°æ®
                with zip_file.open(csv_files[0]) as csv_file:
                    df = pd.read_csv(csv_file, header=None)
                    
                    # è®¾ç½®åˆ—å
                    df.columns = [
                        'open_time', 'open', 'high', 'low', 'close', 'volume',
                        'close_time', 'quote_volume', 'count', 'taker_buy_volume',
                        'taker_buy_quote_volume', 'ignore'
                    ]
                    
                    # è½¬æ¢æ—¶é—´æˆ³ (å¤„ç†å¯èƒ½çš„æ—¶é—´æˆ³æ ¼å¼é—®é¢˜)
                    try:
                        # Binanceçš„æ—¶é—´æˆ³æ˜¯å¾®ç§’çº§ (microseconds)
                        df['open_time'] = pd.to_datetime(df['open_time'], unit='us')
                        df['close_time'] = pd.to_datetime(df['close_time'], unit='us')
                    except Exception as e:
                        try:
                            # å¦‚æœå¾®ç§’è½¬æ¢å¤±è´¥ï¼Œå°è¯•æ¯«ç§’è½¬æ¢
                            logger.warning(f"æ—¶é—´æˆ³å¾®ç§’è½¬æ¢å¤±è´¥ï¼Œå°è¯•æ¯«ç§’è½¬æ¢: {e}")
                            df['open_time'] = pd.to_datetime(df['open_time'], unit='ms')
                            df['close_time'] = pd.to_datetime(df['close_time'], unit='ms')
                        except Exception as e2:
                            # æœ€åå°è¯•ç§’è½¬æ¢
                            logger.warning(f"æ—¶é—´æˆ³æ¯«ç§’è½¬æ¢å¤±è´¥ï¼Œå°è¯•ç§’è½¬æ¢: {e2}")
                            df['open_time'] = pd.to_datetime(df['open_time'], unit='s')
                            df['close_time'] = pd.to_datetime(df['close_time'], unit='s')
                    
                    # è½¬æ¢æ•°å€¼ç±»å‹
                    numeric_columns = ['open', 'high', 'low', 'close', 'volume', 
                                     'quote_volume', 'taker_buy_volume', 'taker_buy_quote_volume']
                    for col in numeric_columns:
                        df[col] = pd.to_numeric(df[col], errors='coerce')
                    
                    return df
                    
        except Exception as e:
            logger.error(f"Error parsing {file_path}: {e}")
            return pd.DataFrame()
    
    def validate_ohlc_data(self, df: pd.DataFrame) -> Dict[str, bool]:
        """éªŒè¯OHLCæ•°æ®çš„åˆç†æ€§"""
        results = {}
        
        # æ£€æŸ¥1: ä»·æ ¼åˆç†æ€§ (High >= Low, Open/Closeåœ¨High/Lowä¹‹é—´)
        price_logic = (
            (df['high'] >= df['low']) & 
            (df['high'] >= df['open']) & 
            (df['high'] >= df['close']) &
            (df['low'] <= df['open']) & 
            (df['low'] <= df['close'])
        )
        results['price_logic'] = price_logic.all()
        
        # æ£€æŸ¥2: ä»·æ ¼ä¸èƒ½ä¸º0æˆ–è´Ÿæ•°
        price_positive = (
            (df['open'] > 0) & 
            (df['high'] > 0) & 
            (df['low'] > 0) & 
            (df['close'] > 0)
        )
        results['price_positive'] = price_positive.all()
        
        # æ£€æŸ¥3: æˆäº¤é‡ä¸èƒ½ä¸ºè´Ÿæ•°
        volume_positive = (df['volume'] >= 0)
        results['volume_positive'] = volume_positive.all()
        
        # æ£€æŸ¥4: æ²¡æœ‰é‡å¤çš„æ—¶é—´æˆ³
        no_duplicates = not df['open_time'].duplicated().any()
        results['no_duplicates'] = no_duplicates
        
        # æ£€æŸ¥5: æ—¶é—´æˆ³æ˜¯è¿ç»­çš„
        if len(df) > 1:
            time_diff = df['open_time'].diff().dt.total_seconds()
            expected_interval = 3600  # 1å°æ—¶ = 3600ç§’
            time_continuous = (time_diff[1:] == expected_interval).all()
            results['time_continuous'] = time_continuous
        else:
            results['time_continuous'] = True
        
        # æ£€æŸ¥6: æ²¡æœ‰å¼‚å¸¸çš„ä»·æ ¼è·³è·ƒ (è¶…è¿‡10%çš„å•å°æ—¶æ¶¨è·Œ)
        price_changes = df['close'].pct_change().abs()
        reasonable_changes = (price_changes < 0.1).all()  # 10%é˜ˆå€¼
        results['reasonable_changes'] = reasonable_changes
        
        return results
    
    def analyze_data_completeness(self, start_date: datetime, end_date: datetime) -> Dict[str, Any]:
        """åˆ†ææ•°æ®å®Œæ•´æ€§"""
        results = {
            'total_files': 0,
            'parsed_files': 0,
            'total_records': 0,
            'date_coverage': {},
            'missing_dates': [],
            'data_quality': {}
        }
        
        # æ£€æŸ¥æ¯ä¸€å¤©çš„æ•°æ®æ–‡ä»¶
        current_date = start_date
        while current_date <= end_date:
            date_str = current_date.strftime("%Y-%m-%d")
            file_path = self.data_path / "klines" / "BTCUSDT" / "1h" / f"BTCUSDT-1h-{date_str}.zip"
            
            results['total_files'] += 1
            
            if file_path.exists():
                # è§£ææ–‡ä»¶
                df = self.parse_klines_file(file_path)
                
                if not df.empty:
                    results['parsed_files'] += 1
                    results['total_records'] += len(df)
                    
                    # éªŒè¯æ•°æ®è´¨é‡
                    quality_checks = self.validate_ohlc_data(df)
                    results['data_quality'][date_str] = quality_checks
                    
                    # è®°å½•æ—¥æœŸè¦†ç›–æƒ…å†µ
                    results['date_coverage'][date_str] = {
                        'records': len(df),
                        'start_time': df['open_time'].min(),
                        'end_time': df['open_time'].max(),
                        'quality_score': sum(quality_checks.values()) / len(quality_checks)
                    }
                else:
                    results['missing_dates'].append(date_str)
            else:
                results['missing_dates'].append(date_str)
            
            current_date += timedelta(days=1)
        
        return results
    
    async def download_extended_data(self, start_year: int = 2020, end_year: int = 2024):
        """ä¸‹è½½æ‰©å±•çš„å†å²æ•°æ®ç”¨äºå›æµ‹"""
        logger.info(f"å¼€å§‹ä¸‹è½½ {start_year}-{end_year} å¹´çš„BTCUSDTå†å²æ•°æ®...")
        
        start_date = datetime(start_year, 1, 1)
        end_date = datetime(end_year, 12, 31)
        
        # ä¸‹è½½ä¸åŒé—´éš”çš„æ•°æ®
        intervals = [
            DataInterval.MINUTE_1,   # 1åˆ†é’Ÿæ•°æ® (æœ€é‡è¦)
            DataInterval.MINUTE_5,   # 5åˆ†é’Ÿæ•°æ®
            DataInterval.HOUR_1,     # 1å°æ—¶æ•°æ®
            DataInterval.DAY_1       # æ—¥æ•°æ®
        ]
        
        download_results = {}
        
        for interval in intervals:
            logger.info(f"ä¸‹è½½ {interval.value} é—´éš”æ•°æ®...")
            
            try:
                await self.downloader.download_klines(
                    'BTCUSDT', interval, start_date, end_date
                )
                
                stats = self.downloader.get_download_statistics()
                download_results[interval.value] = {
                    'files_downloaded': stats['total_downloaded'],
                    'total_size_mb': stats['total_size_mb'],
                    'success_rate': stats['success_rate']
                }
                
                logger.info(f"âœ… {interval.value} æ•°æ®ä¸‹è½½å®Œæˆ: "
                          f"{stats['total_downloaded']} æ–‡ä»¶, "
                          f"{stats['total_size_mb']:.2f} MB")
                
            except Exception as e:
                logger.error(f"âŒ {interval.value} æ•°æ®ä¸‹è½½å¤±è´¥: {e}")
                download_results[interval.value] = {'error': str(e)}
        
        return download_results
    
    def generate_data_report(self, analysis_results: Dict[str, Any]) -> str:
        """ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š"""
        report = []
        report.append("# å†å²æ•°æ®è´¨é‡åˆ†ææŠ¥å‘Š")
        report.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        # æ€»ä½“ç»Ÿè®¡
        report.append("## æ€»ä½“ç»Ÿè®¡")
        report.append(f"- æ€»æ–‡ä»¶æ•°: {analysis_results['total_files']}")
        report.append(f"- æˆåŠŸè§£ææ–‡ä»¶æ•°: {analysis_results['parsed_files']}")
        report.append(f"- æ€»è®°å½•æ•°: {analysis_results['total_records']:,}")
        report.append(f"- ç¼ºå¤±æ—¥æœŸæ•°: {len(analysis_results['missing_dates'])}")
        report.append("")
        
        # æ•°æ®å®Œæ•´æ€§
        if analysis_results['missing_dates']:
            report.append("## ç¼ºå¤±æ•°æ®")
            for date in analysis_results['missing_dates']:
                report.append(f"- {date}")
            report.append("")
        
        # æ•°æ®è´¨é‡
        report.append("## æ•°æ®è´¨é‡æ£€æŸ¥")
        quality_scores = []
        for date, quality in analysis_results['data_quality'].items():
            score = sum(quality.values()) / len(quality)
            quality_scores.append(score)
            
            if score < 1.0:  # æœ‰è´¨é‡é—®é¢˜
                report.append(f"### {date} (è´¨é‡åˆ†: {score:.2f})")
                for check, passed in quality.items():
                    status = "âœ…" if passed else "âŒ"
                    report.append(f"  - {check}: {status}")
        
        if quality_scores:
            avg_quality = sum(quality_scores) / len(quality_scores)
            report.append(f"å¹³å‡è´¨é‡åˆ†: {avg_quality:.2f}")
        
        return "\n".join(report)


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    logger.info("=" * 60)
    logger.info("ğŸ” å†å²æ•°æ®è´¨é‡éªŒè¯æµ‹è¯•")
    logger.info("=" * 60)
    
    validator = HistoricalDataValidator()
    
    # æµ‹è¯•1: éªŒè¯ç°æœ‰æµ‹è¯•æ•°æ®
    logger.info("\nğŸ“Š æµ‹è¯•1: éªŒè¯ç°æœ‰æµ‹è¯•æ•°æ®è´¨é‡")
    logger.info("-" * 40)
    
    test_start = datetime(2025, 7, 4)
    test_end = datetime(2025, 7, 7)
    
    analysis = validator.analyze_data_completeness(test_start, test_end)
    
    # ç”ŸæˆæŠ¥å‘Š
    report = validator.generate_data_report(analysis)
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = Path("./data/data_quality_report.md")
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    logger.info(f"ğŸ“„ æ•°æ®è´¨é‡æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
    
    # è¾“å‡ºå…³é”®ç»Ÿè®¡
    logger.info("ğŸ“Š å…³é”®ç»Ÿè®¡:")
    logger.info(f"  - æ–‡ä»¶è¦†ç›–ç‡: {analysis['parsed_files']}/{analysis['total_files']} ({analysis['parsed_files']/analysis['total_files']*100:.1f}%)")
    logger.info(f"  - æ€»è®°å½•æ•°: {analysis['total_records']:,}")
    logger.info(f"  - ç¼ºå¤±æ—¥æœŸ: {len(analysis['missing_dates'])}")
    
    # è®¡ç®—å¹³å‡è´¨é‡åˆ†
    if analysis['data_quality']:
        quality_scores = []
        for quality in analysis['data_quality'].values():
            score = sum(quality.values()) / len(quality.values())
            quality_scores.append(score)
        avg_quality = sum(quality_scores) / len(quality_scores)
        logger.info(f"  - å¹³å‡è´¨é‡åˆ†: {avg_quality:.2f}/1.00")
    
    # æµ‹è¯•2: æ˜¯å¦éœ€è¦ä¸‹è½½æ›´å¤šæ•°æ®
    logger.info("\nğŸ“¥ æµ‹è¯•2: è¯„ä¼°æ˜¯å¦éœ€è¦ä¸‹è½½æ›´å¤šæ•°æ®")
    logger.info("-" * 40)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œå›æµ‹
    if analysis['total_records'] < 1000:  # å°‘äº1000æ¡è®°å½•
        logger.warning("âš ï¸ å½“å‰æ•°æ®é‡ä¸è¶³ä»¥è¿›è¡Œæœ‰æ•ˆå›æµ‹")
        logger.info("å»ºè®®ä¸‹è½½æ›´å¤šå†å²æ•°æ®...")
        
        # è¯¢é—®æ˜¯å¦ä¸‹è½½æ›´å¤šæ•°æ®
        download_more = input("æ˜¯å¦ä¸‹è½½2022-2024å¹´çš„æ•°æ®? (y/n): ").lower().strip()
        
        if download_more == 'y':
            logger.info("å¼€å§‹ä¸‹è½½2022-2024å¹´æ•°æ®...")
            download_results = await validator.download_extended_data(2022, 2024)
            
            logger.info("ğŸ“Š ä¸‹è½½ç»“æœ:")
            for interval, result in download_results.items():
                if 'error' in result:
                    logger.error(f"  - {interval}: âŒ {result['error']}")
                else:
                    logger.info(f"  - {interval}: âœ… {result['files_downloaded']} æ–‡ä»¶, "
                              f"{result['total_size_mb']:.2f} MB")
        else:
            logger.info("è·³è¿‡æ•°æ®ä¸‹è½½")
    else:
        logger.info("âœ… å½“å‰æ•°æ®é‡è¶³å¤Ÿè¿›è¡ŒåŸºæœ¬å›æµ‹æµ‹è¯•")
    
    # æµ‹è¯•3: æ•°æ®è§£æå’Œé¢„å¤„ç†æµ‹è¯•
    logger.info("\nğŸ”§ æµ‹è¯•3: æ•°æ®è§£æå’Œé¢„å¤„ç†æµ‹è¯•")
    logger.info("-" * 40)
    
    # æµ‹è¯•è§£æä¸€ä¸ªæ–‡ä»¶
    test_file = Path("./data/test_basic/klines/BTCUSDT/1h/BTCUSDT-1h-2025-07-04.zip")
    if test_file.exists():
        df = validator.parse_klines_file(test_file)
        if not df.empty:
            logger.info(f"âœ… æˆåŠŸè§£ææµ‹è¯•æ–‡ä»¶: {len(df)} æ¡è®°å½•")
            logger.info(f"  - æ—¶é—´èŒƒå›´: {df['open_time'].min()} åˆ° {df['open_time'].max()}")
            logger.info(f"  - ä»·æ ¼èŒƒå›´: ${df['low'].min():,.2f} - ${df['high'].max():,.2f}")
            logger.info(f"  - å¹³å‡æˆäº¤é‡: {df['volume'].mean():,.2f}")
            
            # æ•°æ®è´¨é‡æ£€æŸ¥
            quality = validator.validate_ohlc_data(df)
            logger.info("  - æ•°æ®è´¨é‡æ£€æŸ¥:")
            for check, passed in quality.items():
                status = "âœ…" if passed else "âŒ"
                logger.info(f"    {check}: {status}")
        else:
            logger.error("âŒ æ–‡ä»¶è§£æå¤±è´¥")
    else:
        logger.error("âŒ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨")
    
    logger.info("\n" + "=" * 60)
    logger.info("ğŸ¯ å†å²æ•°æ®éªŒè¯å®Œæˆ")
    logger.info("=" * 60)
    
    # æ ¹æ®æµ‹è¯•ç»“æœç»™å‡ºå»ºè®®
    if analysis['parsed_files'] == analysis['total_files'] and analysis['total_records'] > 0:
        logger.info("âœ… æ•°æ®éªŒè¯é€šè¿‡ï¼Œå¯ä»¥è¿›è¡Œä¸‹ä¸€æ­¥å›æµ‹ç³»ç»Ÿå¼€å‘")
        return True
    else:
        logger.error("âŒ æ•°æ®éªŒè¯å¤±è´¥ï¼Œéœ€è¦è§£å†³æ•°æ®é—®é¢˜")
        return False


if __name__ == "__main__":
    try:
        success = asyncio.run(main())
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        logger.info("æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        logger.error(f"æµ‹è¯•ç¨‹åºå‡ºé”™: {e}")
        sys.exit(1)